{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yuki/quant_project/EURUSD-LSTM-prediction\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Navigate to the parent directory\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Append the parent directory to sys.path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Verify that the parent directory was added\n",
    "print(sys.path[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>PX_OPEN</th>\n",
       "      <th>PX_HIGH</th>\n",
       "      <th>PX_LOW</th>\n",
       "      <th>PX_LAST</th>\n",
       "      <th>Last_Return</th>\n",
       "      <th>Predict_Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980/1/3</td>\n",
       "      <td>238.35</td>\n",
       "      <td>238.35</td>\n",
       "      <td>238.35</td>\n",
       "      <td>238.35</td>\n",
       "      <td>-0.000419</td>\n",
       "      <td>-0.014894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980/1/4</td>\n",
       "      <td>234.80</td>\n",
       "      <td>234.80</td>\n",
       "      <td>234.80</td>\n",
       "      <td>234.80</td>\n",
       "      <td>-0.014894</td>\n",
       "      <td>-0.013842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980/1/7</td>\n",
       "      <td>231.55</td>\n",
       "      <td>231.55</td>\n",
       "      <td>231.55</td>\n",
       "      <td>231.55</td>\n",
       "      <td>-0.013842</td>\n",
       "      <td>0.013820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980/1/8</td>\n",
       "      <td>234.75</td>\n",
       "      <td>234.75</td>\n",
       "      <td>234.75</td>\n",
       "      <td>234.75</td>\n",
       "      <td>0.013820</td>\n",
       "      <td>0.000852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1980/1/9</td>\n",
       "      <td>234.95</td>\n",
       "      <td>234.95</td>\n",
       "      <td>234.95</td>\n",
       "      <td>234.95</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.003618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11618</th>\n",
       "      <td>2024/7/15</td>\n",
       "      <td>157.84</td>\n",
       "      <td>158.42</td>\n",
       "      <td>157.19</td>\n",
       "      <td>158.06</td>\n",
       "      <td>0.001457</td>\n",
       "      <td>0.001835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11619</th>\n",
       "      <td>2024/7/16</td>\n",
       "      <td>158.06</td>\n",
       "      <td>158.86</td>\n",
       "      <td>158.00</td>\n",
       "      <td>158.35</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>-0.013578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11620</th>\n",
       "      <td>2024/7/17</td>\n",
       "      <td>158.34</td>\n",
       "      <td>158.61</td>\n",
       "      <td>156.07</td>\n",
       "      <td>156.20</td>\n",
       "      <td>-0.013578</td>\n",
       "      <td>0.007490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11621</th>\n",
       "      <td>2024/7/18</td>\n",
       "      <td>156.20</td>\n",
       "      <td>157.40</td>\n",
       "      <td>155.38</td>\n",
       "      <td>157.37</td>\n",
       "      <td>0.007490</td>\n",
       "      <td>0.000699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11622</th>\n",
       "      <td>2024/7/19</td>\n",
       "      <td>157.37</td>\n",
       "      <td>157.86</td>\n",
       "      <td>156.96</td>\n",
       "      <td>157.48</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>-0.002540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11622 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Dates  PX_OPEN  PX_HIGH  PX_LOW  PX_LAST  Last_Return  \\\n",
       "1       1980/1/3   238.35   238.35  238.35   238.35    -0.000419   \n",
       "2       1980/1/4   234.80   234.80  234.80   234.80    -0.014894   \n",
       "3       1980/1/7   231.55   231.55  231.55   231.55    -0.013842   \n",
       "4       1980/1/8   234.75   234.75  234.75   234.75     0.013820   \n",
       "5       1980/1/9   234.95   234.95  234.95   234.95     0.000852   \n",
       "...          ...      ...      ...     ...      ...          ...   \n",
       "11618  2024/7/15   157.84   158.42  157.19   158.06     0.001457   \n",
       "11619  2024/7/16   158.06   158.86  158.00   158.35     0.001835   \n",
       "11620  2024/7/17   158.34   158.61  156.07   156.20    -0.013578   \n",
       "11621  2024/7/18   156.20   157.40  155.38   157.37     0.007490   \n",
       "11622  2024/7/19   157.37   157.86  156.96   157.48     0.000699   \n",
       "\n",
       "       Predict_Return  \n",
       "1           -0.014894  \n",
       "2           -0.013842  \n",
       "3            0.013820  \n",
       "4            0.000852  \n",
       "5            0.003618  \n",
       "...               ...  \n",
       "11618        0.001835  \n",
       "11619       -0.013578  \n",
       "11620        0.007490  \n",
       "11621        0.000699  \n",
       "11622       -0.002540  \n",
       "\n",
       "[11622 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from data_processing import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "# Set the random seed\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "\n",
    "return_test_day = [1,3,5]\n",
    "prediction_parameters_dic = {\"Forecast period\":1, \"time_rolling_window\":22}\n",
    "\n",
    "\n",
    "df_tech = pd.read_csv(\"../Data/USDJPY_OHLC.csv\")\n",
    "# df_tech[\"Dates\"]=pd.to_datetime(df_tech['Dates'])\n",
    "# df_tech = df_tech[df_tech[\"Dates\"] > pd.to_datetime(\"2002-01-01\") ] \n",
    "\n",
    "df_tech[\"Last_Return\"] =((df_tech[\"PX_LAST\"].pct_change(periods=prediction_parameters_dic[\"Forecast period\"]))\n",
    "                          )\n",
    "\n",
    "df_tech[\"Predict_Return\"] = ((df_tech[\"PX_LAST\"].pct_change(periods=prediction_parameters_dic[\"Forecast period\"])\n",
    "                      .dropna(ignore_index = True)))\n",
    "\n",
    "df_tech.dropna(inplace=True)\n",
    "df_tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>PX_OPEN</th>\n",
       "      <th>PX_HIGH</th>\n",
       "      <th>PX_LOW</th>\n",
       "      <th>PX_LAST</th>\n",
       "      <th>Last_Return</th>\n",
       "      <th>Predict_Return</th>\n",
       "      <th>SMA_10</th>\n",
       "      <th>EMA_5_100</th>\n",
       "      <th>RSI_10</th>\n",
       "      <th>...</th>\n",
       "      <th>MACD_26_12</th>\n",
       "      <th>MACD_12_5</th>\n",
       "      <th>MACD_42_18</th>\n",
       "      <th>ROC_5</th>\n",
       "      <th>Bollinger_Bands_lower_12</th>\n",
       "      <th>Bollinger_Bands_lower_100</th>\n",
       "      <th>Bollinger_Bands_upper_12</th>\n",
       "      <th>Bollinger_Bands_upper_100</th>\n",
       "      <th>CCI_10</th>\n",
       "      <th>CCI_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980/5/21</td>\n",
       "      <td>224.40</td>\n",
       "      <td>224.40</td>\n",
       "      <td>224.40</td>\n",
       "      <td>224.40</td>\n",
       "      <td>-0.010364</td>\n",
       "      <td>-0.001560</td>\n",
       "      <td>228.470</td>\n",
       "      <td>-110004.700000</td>\n",
       "      <td>34.805195</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.612349</td>\n",
       "      <td>-1.674802</td>\n",
       "      <td>-7.855266</td>\n",
       "      <td>-0.014925</td>\n",
       "      <td>223.781396</td>\n",
       "      <td>227.572993</td>\n",
       "      <td>234.276937</td>\n",
       "      <td>258.684007</td>\n",
       "      <td>-134.057529</td>\n",
       "      <td>-194.397015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980/5/22</td>\n",
       "      <td>224.05</td>\n",
       "      <td>224.05</td>\n",
       "      <td>224.05</td>\n",
       "      <td>224.05</td>\n",
       "      <td>-0.001560</td>\n",
       "      <td>-0.012497</td>\n",
       "      <td>227.515</td>\n",
       "      <td>28289.229630</td>\n",
       "      <td>37.640449</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.413818</td>\n",
       "      <td>-1.678434</td>\n",
       "      <td>-8.200525</td>\n",
       "      <td>-0.024172</td>\n",
       "      <td>222.764120</td>\n",
       "      <td>226.999937</td>\n",
       "      <td>234.002546</td>\n",
       "      <td>258.971063</td>\n",
       "      <td>-137.909899</td>\n",
       "      <td>-192.097577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980/5/23</td>\n",
       "      <td>221.25</td>\n",
       "      <td>221.25</td>\n",
       "      <td>221.25</td>\n",
       "      <td>221.25</td>\n",
       "      <td>-0.012497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>226.460</td>\n",
       "      <td>126773.537037</td>\n",
       "      <td>42.948718</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.319445</td>\n",
       "      <td>-2.607804</td>\n",
       "      <td>-8.459722</td>\n",
       "      <td>-0.033843</td>\n",
       "      <td>221.064940</td>\n",
       "      <td>226.367223</td>\n",
       "      <td>233.935060</td>\n",
       "      <td>259.332777</td>\n",
       "      <td>-179.407096</td>\n",
       "      <td>-214.528123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980/5/26</td>\n",
       "      <td>221.25</td>\n",
       "      <td>221.25</td>\n",
       "      <td>221.25</td>\n",
       "      <td>221.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009266</td>\n",
       "      <td>225.905</td>\n",
       "      <td>141133.632716</td>\n",
       "      <td>35.036496</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.608238</td>\n",
       "      <td>-2.828137</td>\n",
       "      <td>-8.359897</td>\n",
       "      <td>-0.024256</td>\n",
       "      <td>220.323414</td>\n",
       "      <td>225.859303</td>\n",
       "      <td>232.618253</td>\n",
       "      <td>259.634697</td>\n",
       "      <td>-122.467449</td>\n",
       "      <td>-210.151438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980/5/27</td>\n",
       "      <td>219.20</td>\n",
       "      <td>219.20</td>\n",
       "      <td>219.20</td>\n",
       "      <td>219.20</td>\n",
       "      <td>-0.009266</td>\n",
       "      <td>0.019617</td>\n",
       "      <td>225.050</td>\n",
       "      <td>10505.295062</td>\n",
       "      <td>31.578947</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.353031</td>\n",
       "      <td>-2.722208</td>\n",
       "      <td>-8.593441</td>\n",
       "      <td>-0.023173</td>\n",
       "      <td>218.976009</td>\n",
       "      <td>225.135327</td>\n",
       "      <td>231.865658</td>\n",
       "      <td>260.047673</td>\n",
       "      <td>-129.138788</td>\n",
       "      <td>-223.374622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11518</th>\n",
       "      <td>2024/7/15</td>\n",
       "      <td>157.84</td>\n",
       "      <td>158.42</td>\n",
       "      <td>157.19</td>\n",
       "      <td>158.06</td>\n",
       "      <td>0.001457</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>160.374</td>\n",
       "      <td>-32237.003333</td>\n",
       "      <td>19.137466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.567377</td>\n",
       "      <td>-0.883627</td>\n",
       "      <td>1.267316</td>\n",
       "      <td>-0.020269</td>\n",
       "      <td>157.793239</td>\n",
       "      <td>147.367981</td>\n",
       "      <td>163.220094</td>\n",
       "      <td>162.486819</td>\n",
       "      <td>-145.549381</td>\n",
       "      <td>65.014890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11519</th>\n",
       "      <td>2024/7/16</td>\n",
       "      <td>158.06</td>\n",
       "      <td>158.86</td>\n",
       "      <td>158.00</td>\n",
       "      <td>158.35</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>-0.013578</td>\n",
       "      <td>160.065</td>\n",
       "      <td>167915.171481</td>\n",
       "      <td>19.571046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.458689</td>\n",
       "      <td>-0.918208</td>\n",
       "      <td>1.150419</td>\n",
       "      <td>-0.020657</td>\n",
       "      <td>157.348151</td>\n",
       "      <td>147.468675</td>\n",
       "      <td>163.243516</td>\n",
       "      <td>162.542925</td>\n",
       "      <td>-86.477867</td>\n",
       "      <td>74.680207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11520</th>\n",
       "      <td>2024/7/17</td>\n",
       "      <td>158.34</td>\n",
       "      <td>158.61</td>\n",
       "      <td>156.07</td>\n",
       "      <td>156.20</td>\n",
       "      <td>-0.013578</td>\n",
       "      <td>0.007490</td>\n",
       "      <td>159.516</td>\n",
       "      <td>61781.160988</td>\n",
       "      <td>15.153088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187489</td>\n",
       "      <td>-1.637167</td>\n",
       "      <td>0.868412</td>\n",
       "      <td>-0.016620</td>\n",
       "      <td>156.243572</td>\n",
       "      <td>147.570365</td>\n",
       "      <td>163.471428</td>\n",
       "      <td>162.551435</td>\n",
       "      <td>-120.766326</td>\n",
       "      <td>42.722246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11521</th>\n",
       "      <td>2024/7/18</td>\n",
       "      <td>156.20</td>\n",
       "      <td>157.40</td>\n",
       "      <td>155.38</td>\n",
       "      <td>157.37</td>\n",
       "      <td>0.007490</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>159.125</td>\n",
       "      <td>-12037.342840</td>\n",
       "      <td>17.952218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013562</td>\n",
       "      <td>-1.599684</td>\n",
       "      <td>0.630174</td>\n",
       "      <td>-0.002915</td>\n",
       "      <td>155.799715</td>\n",
       "      <td>147.700627</td>\n",
       "      <td>163.236952</td>\n",
       "      <td>162.568973</td>\n",
       "      <td>-106.276413</td>\n",
       "      <td>36.481058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11522</th>\n",
       "      <td>2024/7/19</td>\n",
       "      <td>157.37</td>\n",
       "      <td>157.86</td>\n",
       "      <td>156.96</td>\n",
       "      <td>157.48</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>-0.002540</td>\n",
       "      <td>158.798</td>\n",
       "      <td>-25865.816543</td>\n",
       "      <td>18.119891</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215059</td>\n",
       "      <td>-1.256250</td>\n",
       "      <td>0.628587</td>\n",
       "      <td>-0.003669</td>\n",
       "      <td>155.541390</td>\n",
       "      <td>147.828744</td>\n",
       "      <td>162.793610</td>\n",
       "      <td>162.588056</td>\n",
       "      <td>-63.167322</td>\n",
       "      <td>51.276491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11523 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Dates  PX_OPEN  PX_HIGH  PX_LOW  PX_LAST  Last_Return  \\\n",
       "0      1980/5/21   224.40   224.40  224.40   224.40    -0.010364   \n",
       "1      1980/5/22   224.05   224.05  224.05   224.05    -0.001560   \n",
       "2      1980/5/23   221.25   221.25  221.25   221.25    -0.012497   \n",
       "3      1980/5/26   221.25   221.25  221.25   221.25     0.000000   \n",
       "4      1980/5/27   219.20   219.20  219.20   219.20    -0.009266   \n",
       "...          ...      ...      ...     ...      ...          ...   \n",
       "11518  2024/7/15   157.84   158.42  157.19   158.06     0.001457   \n",
       "11519  2024/7/16   158.06   158.86  158.00   158.35     0.001835   \n",
       "11520  2024/7/17   158.34   158.61  156.07   156.20    -0.013578   \n",
       "11521  2024/7/18   156.20   157.40  155.38   157.37     0.007490   \n",
       "11522  2024/7/19   157.37   157.86  156.96   157.48     0.000699   \n",
       "\n",
       "       Predict_Return   SMA_10      EMA_5_100     RSI_10  ...  MACD_26_12  \\\n",
       "0           -0.001560  228.470 -110004.700000  34.805195  ...   -6.612349   \n",
       "1           -0.012497  227.515   28289.229630  37.640449  ...   -6.413818   \n",
       "2            0.000000  226.460  126773.537037  42.948718  ...   -6.319445   \n",
       "3           -0.009266  225.905  141133.632716  35.036496  ...   -6.608238   \n",
       "4            0.019617  225.050   10505.295062  31.578947  ...   -7.353031   \n",
       "...               ...      ...            ...        ...  ...         ...   \n",
       "11518        0.001835  160.374  -32237.003333  19.137466  ...    0.567377   \n",
       "11519       -0.013578  160.065  167915.171481  19.571046  ...    0.458689   \n",
       "11520        0.007490  159.516   61781.160988  15.153088  ...    0.187489   \n",
       "11521        0.000699  159.125  -12037.342840  17.952218  ...    0.013562   \n",
       "11522       -0.002540  158.798  -25865.816543  18.119891  ...   -0.215059   \n",
       "\n",
       "       MACD_12_5  MACD_42_18     ROC_5  Bollinger_Bands_lower_12  \\\n",
       "0      -1.674802   -7.855266 -0.014925                223.781396   \n",
       "1      -1.678434   -8.200525 -0.024172                222.764120   \n",
       "2      -2.607804   -8.459722 -0.033843                221.064940   \n",
       "3      -2.828137   -8.359897 -0.024256                220.323414   \n",
       "4      -2.722208   -8.593441 -0.023173                218.976009   \n",
       "...          ...         ...       ...                       ...   \n",
       "11518  -0.883627    1.267316 -0.020269                157.793239   \n",
       "11519  -0.918208    1.150419 -0.020657                157.348151   \n",
       "11520  -1.637167    0.868412 -0.016620                156.243572   \n",
       "11521  -1.599684    0.630174 -0.002915                155.799715   \n",
       "11522  -1.256250    0.628587 -0.003669                155.541390   \n",
       "\n",
       "       Bollinger_Bands_lower_100  Bollinger_Bands_upper_12  \\\n",
       "0                     227.572993                234.276937   \n",
       "1                     226.999937                234.002546   \n",
       "2                     226.367223                233.935060   \n",
       "3                     225.859303                232.618253   \n",
       "4                     225.135327                231.865658   \n",
       "...                          ...                       ...   \n",
       "11518                 147.367981                163.220094   \n",
       "11519                 147.468675                163.243516   \n",
       "11520                 147.570365                163.471428   \n",
       "11521                 147.700627                163.236952   \n",
       "11522                 147.828744                162.793610   \n",
       "\n",
       "       Bollinger_Bands_upper_100      CCI_10     CCI_100  \n",
       "0                     258.684007 -134.057529 -194.397015  \n",
       "1                     258.971063 -137.909899 -192.097577  \n",
       "2                     259.332777 -179.407096 -214.528123  \n",
       "3                     259.634697 -122.467449 -210.151438  \n",
       "4                     260.047673 -129.138788 -223.374622  \n",
       "...                          ...         ...         ...  \n",
       "11518                 162.486819 -145.549381   65.014890  \n",
       "11519                 162.542925  -86.477867   74.680207  \n",
       "11520                 162.551435 -120.766326   42.722246  \n",
       "11521                 162.568973 -106.276413   36.481058  \n",
       "11522                 162.588056  -63.167322   51.276491  \n",
       "\n",
       "[11523 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# sys.path.append(\"./factors\")\n",
    "# import tech_indicators\n",
    "from factors import tech_indicators\n",
    "\n",
    "USDJPY_close = df_tech['PX_LAST'].to_numpy()\n",
    "USDJPY_typical = ((df_tech['PX_HIGH']+df_tech['PX_LOW']+df_tech['PX_LAST'])/3).to_numpy()\n",
    "\n",
    "tech_dict =( {\"SMA\":[[10]],\"EMA\":[[5,100]],\"RSI\":[[10],[30],[100]],\"MACD\":[[26,12],[12,5],[42,18]],\"ROC\":[[5]],\"Bollinger_Bands_lower\":[[12],[100]],\n",
    "             \"Bollinger_Bands_upper\":[[12],[100]],\n",
    "              \"CCI\":[[10],[100]]} )\n",
    "\n",
    "for key,value in tech_dict.items():\n",
    "    \n",
    "    for value in value:\n",
    "        df_column_name = key\n",
    "        for para in value:\n",
    "            df_column_name += (\"_\"+str(para))\n",
    "        \n",
    "        if key[:3] == \"CCI\":\n",
    "            df_tech[df_column_name] = tech_indicators.techindicator_to_array(USDJPY_typical,getattr(tech_indicators,key),*value)\n",
    "            \n",
    "        else:\n",
    "            df_tech[df_column_name] = tech_indicators.techindicator_to_array(USDJPY_close,getattr(tech_indicators,key),*value)\n",
    "        \n",
    "df_tech.dropna(inplace=True,ignore_index=True)\n",
    "\n",
    "\n",
    "df_tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002039\n"
     ]
    }
   ],
   "source": [
    "split_point = int(df_tech.shape[0]*0.9)\n",
    "df_train = df_tech.iloc[:split_point,:]\n",
    "df_test = df_tech.iloc[split_point:,:]\n",
    "\n",
    "x_train = df_train.drop(columns=[\"Dates\",\"Predict_Return\"]).to_numpy()\n",
    "y_train = df_train.loc[:, \"Predict_Return\"].to_numpy()\n",
    "\n",
    "x_test = df_test.drop(columns=[\"Dates\",\"Predict_Return\"]).to_numpy()\n",
    "y_test = df_test.loc[:, \"Predict_Return\"].to_numpy()\n",
    "\n",
    "scale_x = StandardScaler()\n",
    "\n",
    "x_train_norm = scale_x.fit_transform(x_train)\n",
    "x_test_norm = scale_x.transform(x_test)\n",
    "\n",
    "time_delta = prediction_parameters_dic[\"time_rolling_window\"]\n",
    "x_train_norm_rolling,x_test_norm_rolling = rolling_split(x_train_norm,time_delta),rolling_split(x_test_norm,time_delta)\n",
    "y_train_rolling,y_test_rolling = y_train[time_delta-1:,...],y_test[time_delta-1:,...]\n",
    "\n",
    "threshold = threshold_search(y_train,1e-6)\n",
    "\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rolling_label , y_test_rolling_label  = labelize(y_train_rolling,threshold),labelize(y_test_rolling,threshold) \n",
    "\n",
    "train_set = torch.utils.data.TensorDataset(torch.from_numpy(x_train_norm_rolling),torch.from_numpy(y_train_rolling_label).to(torch.int64))\n",
    "\n",
    "test_set = torch.utils.data.TensorDataset(torch.from_numpy(x_test_norm_rolling),torch.from_numpy(y_test_rolling_label).to(torch.int64))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuki/miniconda3/envs/initial/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.75 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "import time_net\n",
    "\n",
    "hyperparas = {'input_dim':x_train.shape[-1],'hidden_dim':32,'hidden_nums':5,'output_dim':3,'block_layer_nums':2, 'LSTM_layer_nums':1\n",
    "        , 'dropout_rate':0.75}\n",
    "\n",
    "net_test = time_net.LSTM_Net(hyperparas=hyperparas)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "net_test.to(device=device,dtype=torch.float64)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train_epoch(loss_function, optimizer, model, loader,train_data,test_data):\n",
    "  loss_train = 0\n",
    "  loss_test = 0\n",
    "  \n",
    "  for(i, (x, y)) in enumerate(loader):\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x=x.to(device=device)\n",
    "    y=y.to(device=device)\n",
    "    # Run a forward pass\n",
    "    outputs = model.forward(x)\n",
    "    # Compute the batch loss\n",
    "    loss = loss_function(outputs,y)\n",
    "    # Calculate the gradients\n",
    "    loss.backward()\n",
    "    # Update the parameteres\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "  with torch.no_grad():\n",
    "    train_outputs = model.forward(train_data[:][0].to(device=device))\n",
    "    train_loss = loss_function(train_outputs,train_data[:][1].to(device=device))\n",
    "    loss_train = train_loss.detach().cpu()\n",
    "    print(f\"train loss is {train_loss}\")\n",
    "    \n",
    "    test_outputs = model.forward(test_data[:][0].to(device=device))\n",
    "    test_loss = loss_function(test_outputs,test_data[:][1].to(device=device))\n",
    "    loss_test = test_loss.detach().cpu()\n",
    "    \n",
    "    print(f\"test loss is {test_loss}\")\n",
    "        \n",
    "  return [loss_train, loss_test]   \n",
    "\n",
    "\n",
    "\n",
    "def train_model(loss_function, optimizer, model, loader,train_data,test_data,epochs=25):\n",
    "  loss_ls = []\n",
    "  \n",
    "  for i in range(epochs):\n",
    "    print(f\"-----------------------Epoch: {i+1}----------------------------------\")\n",
    "\n",
    "    loss_ls.append(train_epoch(loss_function, optimizer, model, loader,train_data,test_data))\n",
    "    \n",
    "  return loss_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Epoch: 1----------------------------------\n",
      "train loss is 1.5785040920131772\n",
      "test loss is 1.5656585714571034\n",
      "-----------------------Epoch: 2----------------------------------\n",
      "train loss is 1.4102507715323933\n",
      "test loss is 1.4065646833953798\n",
      "-----------------------Epoch: 3----------------------------------\n",
      "train loss is 1.3152948351455127\n",
      "test loss is 1.2875542805745046\n",
      "-----------------------Epoch: 4----------------------------------\n",
      "train loss is 1.2368627994960082\n",
      "test loss is 1.226982826902457\n",
      "-----------------------Epoch: 5----------------------------------\n",
      "train loss is 1.20872485907785\n",
      "test loss is 1.2219639894322016\n",
      "-----------------------Epoch: 6----------------------------------\n",
      "train loss is 1.175138012627389\n",
      "test loss is 1.1835355156837892\n",
      "-----------------------Epoch: 7----------------------------------\n",
      "train loss is 1.145078420395749\n",
      "test loss is 1.153633141238059\n",
      "-----------------------Epoch: 8----------------------------------\n",
      "train loss is 1.1370436302683102\n",
      "test loss is 1.148379689897125\n",
      "-----------------------Epoch: 9----------------------------------\n",
      "train loss is 1.1302175539964312\n",
      "test loss is 1.1289771763736625\n",
      "-----------------------Epoch: 10----------------------------------\n",
      "train loss is 1.124253740238861\n",
      "test loss is 1.1233717573354816\n",
      "-----------------------Epoch: 11----------------------------------\n",
      "train loss is 1.1163564041713154\n",
      "test loss is 1.1152673692690884\n",
      "-----------------------Epoch: 12----------------------------------\n",
      "train loss is 1.110198896944414\n",
      "test loss is 1.103739386673074\n",
      "-----------------------Epoch: 13----------------------------------\n",
      "train loss is 1.1083768922853203\n",
      "test loss is 1.1075853681859429\n",
      "-----------------------Epoch: 14----------------------------------\n",
      "train loss is 1.1081158208359443\n",
      "test loss is 1.1034029028098316\n",
      "-----------------------Epoch: 15----------------------------------\n",
      "train loss is 1.1022152900238276\n",
      "test loss is 1.1082613892697186\n",
      "-----------------------Epoch: 16----------------------------------\n",
      "train loss is 1.10540417146616\n",
      "test loss is 1.101732175409625\n",
      "-----------------------Epoch: 17----------------------------------\n",
      "train loss is 1.1000031219160387\n",
      "test loss is 1.1042605862172619\n",
      "-----------------------Epoch: 18----------------------------------\n",
      "train loss is 1.1003059407459035\n",
      "test loss is 1.0938418761125528\n",
      "-----------------------Epoch: 19----------------------------------\n",
      "train loss is 1.1011078273109884\n",
      "test loss is 1.0997613499605863\n",
      "-----------------------Epoch: 20----------------------------------\n",
      "train loss is 1.0999154148444774\n",
      "test loss is 1.1007520430492987\n",
      "-----------------------Epoch: 21----------------------------------\n",
      "train loss is 1.0986162273047364\n",
      "test loss is 1.0986108906103524\n",
      "-----------------------Epoch: 22----------------------------------\n",
      "train loss is 1.1002745005061467\n",
      "test loss is 1.0995668006424586\n",
      "-----------------------Epoch: 23----------------------------------\n",
      "train loss is 1.0983353449403492\n",
      "test loss is 1.095213934624688\n",
      "-----------------------Epoch: 24----------------------------------\n",
      "train loss is 1.0965703025274056\n",
      "test loss is 1.0934504999823542\n",
      "-----------------------Epoch: 25----------------------------------\n",
      "train loss is 1.097992483144984\n",
      "test loss is 1.0980367598712364\n",
      "-----------------------Epoch: 26----------------------------------\n",
      "train loss is 1.0958057783772561\n",
      "test loss is 1.0946085907329433\n",
      "-----------------------Epoch: 27----------------------------------\n",
      "train loss is 1.098122383550969\n",
      "test loss is 1.0939148005206425\n",
      "-----------------------Epoch: 28----------------------------------\n",
      "train loss is 1.0972923722181676\n",
      "test loss is 1.0970571194552292\n",
      "-----------------------Epoch: 29----------------------------------\n",
      "train loss is 1.0968512828238601\n",
      "test loss is 1.0934944999917957\n",
      "-----------------------Epoch: 30----------------------------------\n",
      "train loss is 1.096196334420804\n",
      "test loss is 1.0986806253155526\n",
      "-----------------------Epoch: 31----------------------------------\n",
      "train loss is 1.0960622048396642\n",
      "test loss is 1.1007784932255267\n",
      "-----------------------Epoch: 32----------------------------------\n",
      "train loss is 1.0955034840351778\n",
      "test loss is 1.0986501043434664\n",
      "-----------------------Epoch: 33----------------------------------\n",
      "train loss is 1.09495800507654\n",
      "test loss is 1.0961786269698162\n",
      "-----------------------Epoch: 34----------------------------------\n",
      "train loss is 1.0945212786210232\n",
      "test loss is 1.0962651873501397\n",
      "-----------------------Epoch: 35----------------------------------\n",
      "train loss is 1.0949035938892484\n",
      "test loss is 1.0964440231653552\n",
      "-----------------------Epoch: 36----------------------------------\n",
      "train loss is 1.0948271389602902\n",
      "test loss is 1.0924842480512755\n",
      "-----------------------Epoch: 37----------------------------------\n",
      "train loss is 1.0923581393577373\n",
      "test loss is 1.0931146389246238\n",
      "-----------------------Epoch: 38----------------------------------\n",
      "train loss is 1.0945704733552424\n",
      "test loss is 1.0992461415840062\n",
      "-----------------------Epoch: 39----------------------------------\n",
      "train loss is 1.0931874049288435\n",
      "test loss is 1.0940367533820885\n",
      "-----------------------Epoch: 40----------------------------------\n",
      "train loss is 1.0942861099969459\n",
      "test loss is 1.0979286195167204\n",
      "-----------------------Epoch: 41----------------------------------\n",
      "train loss is 1.0922312162483094\n",
      "test loss is 1.094801706067153\n",
      "-----------------------Epoch: 42----------------------------------\n",
      "train loss is 1.092709410367505\n",
      "test loss is 1.0907231183694865\n",
      "-----------------------Epoch: 43----------------------------------\n",
      "train loss is 1.0935402737633186\n",
      "test loss is 1.1006955826318365\n",
      "-----------------------Epoch: 44----------------------------------\n",
      "train loss is 1.0919664404683005\n",
      "test loss is 1.0966904494801113\n",
      "-----------------------Epoch: 45----------------------------------\n",
      "train loss is 1.0909147430805557\n",
      "test loss is 1.0902288382910394\n",
      "-----------------------Epoch: 46----------------------------------\n",
      "train loss is 1.0904715618858984\n",
      "test loss is 1.0976293726886537\n",
      "-----------------------Epoch: 47----------------------------------\n",
      "train loss is 1.0915693032499691\n",
      "test loss is 1.0915445048918455\n",
      "-----------------------Epoch: 48----------------------------------\n",
      "train loss is 1.0888378836976023\n",
      "test loss is 1.0900342734933428\n",
      "-----------------------Epoch: 49----------------------------------\n",
      "train loss is 1.0899381480308912\n",
      "test loss is 1.0947234006798743\n",
      "-----------------------Epoch: 50----------------------------------\n",
      "train loss is 1.0892482925475926\n",
      "test loss is 1.0932628178599064\n",
      "-----------------------Epoch: 51----------------------------------\n",
      "train loss is 1.0895140772508005\n",
      "test loss is 1.0925621943566952\n",
      "-----------------------Epoch: 52----------------------------------\n",
      "train loss is 1.0902368255220432\n",
      "test loss is 1.09398481256419\n",
      "-----------------------Epoch: 53----------------------------------\n",
      "train loss is 1.0883650370668245\n",
      "test loss is 1.0878973411981312\n",
      "-----------------------Epoch: 54----------------------------------\n",
      "train loss is 1.087010451382092\n",
      "test loss is 1.089804142200262\n",
      "-----------------------Epoch: 55----------------------------------\n",
      "train loss is 1.0873052238183034\n",
      "test loss is 1.0929123192671495\n",
      "-----------------------Epoch: 56----------------------------------\n",
      "train loss is 1.0879621052828552\n",
      "test loss is 1.0943694003810502\n",
      "-----------------------Epoch: 57----------------------------------\n",
      "train loss is 1.0879197514461996\n",
      "test loss is 1.0968703406997302\n",
      "-----------------------Epoch: 58----------------------------------\n",
      "train loss is 1.087559191499022\n",
      "test loss is 1.08973353418245\n",
      "-----------------------Epoch: 59----------------------------------\n",
      "train loss is 1.0857079373105318\n",
      "test loss is 1.0909854705535325\n",
      "-----------------------Epoch: 60----------------------------------\n",
      "train loss is 1.087172779898873\n",
      "test loss is 1.0905868984738214\n",
      "-----------------------Epoch: 61----------------------------------\n",
      "train loss is 1.08738516507704\n",
      "test loss is 1.0995400438210934\n",
      "-----------------------Epoch: 62----------------------------------\n",
      "train loss is 1.085152158711997\n",
      "test loss is 1.0919475067536497\n",
      "-----------------------Epoch: 63----------------------------------\n",
      "train loss is 1.0854185132346024\n",
      "test loss is 1.0851592756689807\n",
      "-----------------------Epoch: 64----------------------------------\n",
      "train loss is 1.0849346245370526\n",
      "test loss is 1.0845911861378315\n",
      "-----------------------Epoch: 65----------------------------------\n",
      "train loss is 1.0844643576037116\n",
      "test loss is 1.0914255302479086\n",
      "-----------------------Epoch: 66----------------------------------\n",
      "train loss is 1.085071050708905\n",
      "test loss is 1.0870347030943108\n",
      "-----------------------Epoch: 67----------------------------------\n",
      "train loss is 1.082926245073941\n",
      "test loss is 1.0875334491300181\n",
      "-----------------------Epoch: 68----------------------------------\n",
      "train loss is 1.084434337515741\n",
      "test loss is 1.0939643565313983\n",
      "-----------------------Epoch: 69----------------------------------\n",
      "train loss is 1.0839486820492858\n",
      "test loss is 1.090304710084181\n",
      "-----------------------Epoch: 70----------------------------------\n",
      "train loss is 1.0829984017804795\n",
      "test loss is 1.0911184550873163\n",
      "-----------------------Epoch: 71----------------------------------\n",
      "train loss is 1.0820798974978452\n",
      "test loss is 1.088854470898338\n",
      "-----------------------Epoch: 72----------------------------------\n",
      "train loss is 1.0818469778657214\n",
      "test loss is 1.0860455545380079\n",
      "-----------------------Epoch: 73----------------------------------\n",
      "train loss is 1.0818956448064783\n",
      "test loss is 1.083526560991003\n",
      "-----------------------Epoch: 74----------------------------------\n",
      "train loss is 1.080236974309654\n",
      "test loss is 1.09081099091321\n",
      "-----------------------Epoch: 75----------------------------------\n",
      "train loss is 1.081876382734565\n",
      "test loss is 1.0846087005720866\n",
      "-----------------------Epoch: 76----------------------------------\n",
      "train loss is 1.0815694074850801\n",
      "test loss is 1.0826152542131227\n",
      "-----------------------Epoch: 77----------------------------------\n",
      "train loss is 1.0817035939534205\n",
      "test loss is 1.086463102492219\n",
      "-----------------------Epoch: 78----------------------------------\n",
      "train loss is 1.08191723219704\n",
      "test loss is 1.085156058752098\n",
      "-----------------------Epoch: 79----------------------------------\n",
      "train loss is 1.0819765745948327\n",
      "test loss is 1.0901197917470458\n",
      "-----------------------Epoch: 80----------------------------------\n",
      "train loss is 1.0804718965746227\n",
      "test loss is 1.086710715860916\n",
      "-----------------------Epoch: 81----------------------------------\n",
      "train loss is 1.0807214630972566\n",
      "test loss is 1.087449844948369\n",
      "-----------------------Epoch: 82----------------------------------\n",
      "train loss is 1.0793227681791087\n",
      "test loss is 1.0872046149690873\n",
      "-----------------------Epoch: 83----------------------------------\n",
      "train loss is 1.0795592368061877\n",
      "test loss is 1.084611359784986\n",
      "-----------------------Epoch: 84----------------------------------\n",
      "train loss is 1.080271202371039\n",
      "test loss is 1.0845190408298362\n",
      "-----------------------Epoch: 85----------------------------------\n",
      "train loss is 1.079865856145485\n",
      "test loss is 1.0875854658578534\n",
      "-----------------------Epoch: 86----------------------------------\n",
      "train loss is 1.0792679083729826\n",
      "test loss is 1.0865022645037634\n",
      "-----------------------Epoch: 87----------------------------------\n",
      "train loss is 1.079964295219174\n",
      "test loss is 1.0884843180429347\n",
      "-----------------------Epoch: 88----------------------------------\n",
      "train loss is 1.0785786787630274\n",
      "test loss is 1.0844807785558948\n",
      "-----------------------Epoch: 89----------------------------------\n",
      "train loss is 1.078478585757041\n",
      "test loss is 1.0866373466184986\n",
      "-----------------------Epoch: 90----------------------------------\n",
      "train loss is 1.078065999233718\n",
      "test loss is 1.0804585796838677\n",
      "-----------------------Epoch: 91----------------------------------\n",
      "train loss is 1.0798012834571136\n",
      "test loss is 1.0835087059223694\n",
      "-----------------------Epoch: 92----------------------------------\n",
      "train loss is 1.0769429583984274\n",
      "test loss is 1.089118022032569\n",
      "-----------------------Epoch: 93----------------------------------\n",
      "train loss is 1.079072383733492\n",
      "test loss is 1.0813884006209529\n",
      "-----------------------Epoch: 94----------------------------------\n",
      "train loss is 1.0759938338100101\n",
      "test loss is 1.0905851407939597\n",
      "-----------------------Epoch: 95----------------------------------\n",
      "train loss is 1.0766826928167026\n",
      "test loss is 1.0872858907459204\n",
      "-----------------------Epoch: 96----------------------------------\n",
      "train loss is 1.0776740269483518\n",
      "test loss is 1.0833523480074925\n",
      "-----------------------Epoch: 97----------------------------------\n",
      "train loss is 1.0770935780216564\n",
      "test loss is 1.0846070702881767\n",
      "-----------------------Epoch: 98----------------------------------\n",
      "train loss is 1.0761350051513356\n",
      "test loss is 1.0854876570195826\n",
      "-----------------------Epoch: 99----------------------------------\n",
      "train loss is 1.0770719042723866\n",
      "test loss is 1.0806357099872614\n",
      "-----------------------Epoch: 100----------------------------------\n",
      "train loss is 1.076564685050342\n",
      "test loss is 1.0828320619798257\n"
     ]
    }
   ],
   "source": [
    "optim_Adam = torch.optim.Adam(net_test.parameters(),lr = 0.0001)\n",
    "epochs = 100\n",
    "loss_ls = train_model(loss_function=loss,optimizer=optim_Adam,model=net_test,loader=train_loader,train_data=train_set,\n",
    "                      test_data=test_set,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuki/miniconda3/envs/initial/lib/python3.12/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/home/yuki/miniconda3/envs/initial/lib/python3.12/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_df = pd.DataFrame(data=np.array(loss_ls),columns=[\"Train_loss\", \"Test_loss\"])\n",
    "seaborn.lineplot(data=loss_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41519434628975266"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "net_test.to(device=torch.device(\"cpu\"))\n",
    "net_test.eval()\n",
    "\n",
    "predict_probability = torch.max(F.softmax(net_test(test_set[:][0]),dim=-1),dim=-1)[0]\n",
    "\n",
    "predict_results = torch.argmax(F.softmax(net_test(test_set[:][0]),dim=-1),dim=-1)\n",
    "real_results = test_set[:][1]\n",
    "\n",
    "accuracy_score(real_results,predict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4061433447098976"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_predict_bool = (predict_results != 1) & (predict_probability > 0.35)\n",
    "\n",
    "act_predict_results = predict_results[act_predict_bool]\n",
    "act_real_results = real_results[act_predict_bool]\n",
    "\n",
    "accuracy_score(act_predict_results,act_real_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([293])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_predict_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_path = \"../trained_model/USDJPY_Tec_LSTM.pth\"\n",
    "\n",
    "torch.save(net_test.state_dict(),saved_path )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "initial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
