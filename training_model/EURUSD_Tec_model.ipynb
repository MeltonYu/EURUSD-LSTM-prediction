{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yuki/quant_project/EURUSD-LSTM-prediction\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Navigate to the parent directory\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Append the parent directory to sys.path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Verify that the parent directory was added\n",
    "print(sys.path[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>PX_OPEN</th>\n",
       "      <th>PX_HIGH</th>\n",
       "      <th>PX_LOW</th>\n",
       "      <th>PX_LAST</th>\n",
       "      <th>Last_Return</th>\n",
       "      <th>Predict_Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/3/1980</td>\n",
       "      <td>1.5177</td>\n",
       "      <td>1.5177</td>\n",
       "      <td>1.5177</td>\n",
       "      <td>1.5177</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>-0.003163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/4/1980</td>\n",
       "      <td>1.5129</td>\n",
       "      <td>1.5129</td>\n",
       "      <td>1.5129</td>\n",
       "      <td>1.5129</td>\n",
       "      <td>-0.003163</td>\n",
       "      <td>0.002578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/7/1980</td>\n",
       "      <td>1.5168</td>\n",
       "      <td>1.5168</td>\n",
       "      <td>1.5168</td>\n",
       "      <td>1.5168</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>-0.003956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/8/1980</td>\n",
       "      <td>1.5108</td>\n",
       "      <td>1.5108</td>\n",
       "      <td>1.5108</td>\n",
       "      <td>1.5108</td>\n",
       "      <td>-0.003956</td>\n",
       "      <td>0.003508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1/9/1980</td>\n",
       "      <td>1.5161</td>\n",
       "      <td>1.5161</td>\n",
       "      <td>1.5161</td>\n",
       "      <td>1.5161</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>-0.001847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11466</th>\n",
       "      <td>7/15/2024</td>\n",
       "      <td>1.0903</td>\n",
       "      <td>1.0922</td>\n",
       "      <td>1.0882</td>\n",
       "      <td>1.0894</td>\n",
       "      <td>-0.001192</td>\n",
       "      <td>0.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11467</th>\n",
       "      <td>7/16/2024</td>\n",
       "      <td>1.0894</td>\n",
       "      <td>1.0905</td>\n",
       "      <td>1.0872</td>\n",
       "      <td>1.0899</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.003670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11468</th>\n",
       "      <td>7/17/2024</td>\n",
       "      <td>1.0899</td>\n",
       "      <td>1.0948</td>\n",
       "      <td>1.0895</td>\n",
       "      <td>1.0939</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>-0.003839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11469</th>\n",
       "      <td>7/18/2024</td>\n",
       "      <td>1.0939</td>\n",
       "      <td>1.0941</td>\n",
       "      <td>1.0894</td>\n",
       "      <td>1.0897</td>\n",
       "      <td>-0.003839</td>\n",
       "      <td>-0.001377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11470</th>\n",
       "      <td>7/19/2024</td>\n",
       "      <td>1.0897</td>\n",
       "      <td>1.0902</td>\n",
       "      <td>1.0876</td>\n",
       "      <td>1.0882</td>\n",
       "      <td>-0.001377</td>\n",
       "      <td>-0.000092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11470 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Dates  PX_OPEN  PX_HIGH  PX_LOW  PX_LAST  Last_Return  \\\n",
       "1       1/3/1980   1.5177   1.5177  1.5177   1.5177     0.001848   \n",
       "2       1/4/1980   1.5129   1.5129  1.5129   1.5129    -0.003163   \n",
       "3       1/7/1980   1.5168   1.5168  1.5168   1.5168     0.002578   \n",
       "4       1/8/1980   1.5108   1.5108  1.5108   1.5108    -0.003956   \n",
       "5       1/9/1980   1.5161   1.5161  1.5161   1.5161     0.003508   \n",
       "...          ...      ...      ...     ...      ...          ...   \n",
       "11466  7/15/2024   1.0903   1.0922  1.0882   1.0894    -0.001192   \n",
       "11467  7/16/2024   1.0894   1.0905  1.0872   1.0899     0.000459   \n",
       "11468  7/17/2024   1.0899   1.0948  1.0895   1.0939     0.003670   \n",
       "11469  7/18/2024   1.0939   1.0941  1.0894   1.0897    -0.003839   \n",
       "11470  7/19/2024   1.0897   1.0902  1.0876   1.0882    -0.001377   \n",
       "\n",
       "       Predict_Return  \n",
       "1           -0.003163  \n",
       "2            0.002578  \n",
       "3           -0.003956  \n",
       "4            0.003508  \n",
       "5           -0.001847  \n",
       "...               ...  \n",
       "11466        0.000459  \n",
       "11467        0.003670  \n",
       "11468       -0.003839  \n",
       "11469       -0.001377  \n",
       "11470       -0.000092  \n",
       "\n",
       "[11470 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from data_processing import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "# Set the random seed\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "\n",
    "return_test_day = [1,3,5]\n",
    "prediction_parameters_dic = {\"Forecast period\":1, \"time_rolling_window\":22}\n",
    "\n",
    "\n",
    "df_tech = pd.read_csv(\"../Data/EURUSD_OHLC.csv\")\n",
    "# df_tech[\"Dates\"]=pd.to_datetime(df_tech['Dates'])\n",
    "# df_tech = df_tech[df_tech[\"Dates\"] > pd.to_datetime(\"2002-01-01\") ] \n",
    "\n",
    "df_tech[\"Last_Return\"] =((df_tech[\"PX_LAST\"].pct_change(periods=prediction_parameters_dic[\"Forecast period\"]))\n",
    "                          )\n",
    "\n",
    "df_tech[\"Predict_Return\"] = ((df_tech[\"PX_LAST\"].pct_change(periods=prediction_parameters_dic[\"Forecast period\"])\n",
    "                      .dropna(ignore_index = True)))\n",
    "\n",
    "df_tech.dropna(inplace=True)\n",
    "df_tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>PX_OPEN</th>\n",
       "      <th>PX_HIGH</th>\n",
       "      <th>PX_LOW</th>\n",
       "      <th>PX_LAST</th>\n",
       "      <th>Last_Return</th>\n",
       "      <th>Predict_Return</th>\n",
       "      <th>SMA_10</th>\n",
       "      <th>EMA_5_100</th>\n",
       "      <th>RSI_5</th>\n",
       "      <th>...</th>\n",
       "      <th>MACD_12_5</th>\n",
       "      <th>MACD_42_18</th>\n",
       "      <th>ROC_5</th>\n",
       "      <th>Bollinger_Bands_lower_12</th>\n",
       "      <th>Bollinger_Bands_lower_100</th>\n",
       "      <th>Bollinger_Bands_upper_12</th>\n",
       "      <th>Bollinger_Bands_upper_100</th>\n",
       "      <th>CCI_5</th>\n",
       "      <th>CCI_22</th>\n",
       "      <th>CCI_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5/27/1980</td>\n",
       "      <td>1.4725</td>\n",
       "      <td>1.4725</td>\n",
       "      <td>1.4725</td>\n",
       "      <td>1.4725</td>\n",
       "      <td>0.008354</td>\n",
       "      <td>-0.003328</td>\n",
       "      <td>1.45190</td>\n",
       "      <td>-691.163309</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007195</td>\n",
       "      <td>0.024641</td>\n",
       "      <td>0.021151</td>\n",
       "      <td>1.432406</td>\n",
       "      <td>1.354205</td>\n",
       "      <td>1.468094</td>\n",
       "      <td>1.549607</td>\n",
       "      <td>134.258485</td>\n",
       "      <td>274.066780</td>\n",
       "      <td>34.043608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5/28/1980</td>\n",
       "      <td>1.4676</td>\n",
       "      <td>1.4676</td>\n",
       "      <td>1.4676</td>\n",
       "      <td>1.4676</td>\n",
       "      <td>-0.003328</td>\n",
       "      <td>-0.000886</td>\n",
       "      <td>1.45396</td>\n",
       "      <td>-1.090883</td>\n",
       "      <td>56.379822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007560</td>\n",
       "      <td>0.024186</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>1.434823</td>\n",
       "      <td>1.354549</td>\n",
       "      <td>1.470910</td>\n",
       "      <td>1.548261</td>\n",
       "      <td>59.227299</td>\n",
       "      <td>191.132469</td>\n",
       "      <td>27.108459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5/29/1980</td>\n",
       "      <td>1.4663</td>\n",
       "      <td>1.4663</td>\n",
       "      <td>1.4663</td>\n",
       "      <td>1.4663</td>\n",
       "      <td>-0.000886</td>\n",
       "      <td>-0.001159</td>\n",
       "      <td>1.45564</td>\n",
       "      <td>-335.993816</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007066</td>\n",
       "      <td>0.024720</td>\n",
       "      <td>0.008529</td>\n",
       "      <td>1.435234</td>\n",
       "      <td>1.354825</td>\n",
       "      <td>1.473583</td>\n",
       "      <td>1.547053</td>\n",
       "      <td>25.847759</td>\n",
       "      <td>154.692263</td>\n",
       "      <td>26.016046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5/30/1980</td>\n",
       "      <td>1.4646</td>\n",
       "      <td>1.4646</td>\n",
       "      <td>1.4646</td>\n",
       "      <td>1.4646</td>\n",
       "      <td>-0.001159</td>\n",
       "      <td>-0.001024</td>\n",
       "      <td>1.45715</td>\n",
       "      <td>-752.049206</td>\n",
       "      <td>82.247191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006509</td>\n",
       "      <td>0.027378</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>1.436500</td>\n",
       "      <td>1.355177</td>\n",
       "      <td>1.475250</td>\n",
       "      <td>1.545657</td>\n",
       "      <td>-36.228721</td>\n",
       "      <td>118.693172</td>\n",
       "      <td>24.332424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/2/1980</td>\n",
       "      <td>1.4631</td>\n",
       "      <td>1.4631</td>\n",
       "      <td>1.4631</td>\n",
       "      <td>1.4631</td>\n",
       "      <td>-0.001024</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>1.45876</td>\n",
       "      <td>292.048549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006710</td>\n",
       "      <td>0.028643</td>\n",
       "      <td>-0.006384</td>\n",
       "      <td>1.437667</td>\n",
       "      <td>1.355439</td>\n",
       "      <td>1.476350</td>\n",
       "      <td>1.544441</td>\n",
       "      <td>-95.728255</td>\n",
       "      <td>93.544744</td>\n",
       "      <td>22.846588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11366</th>\n",
       "      <td>7/15/2024</td>\n",
       "      <td>1.0903</td>\n",
       "      <td>1.0922</td>\n",
       "      <td>1.0882</td>\n",
       "      <td>1.0894</td>\n",
       "      <td>-0.001192</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>1.08319</td>\n",
       "      <td>-87.689132</td>\n",
       "      <td>70.676692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>-0.000664</td>\n",
       "      <td>0.007491</td>\n",
       "      <td>1.069854</td>\n",
       "      <td>1.063786</td>\n",
       "      <td>1.093012</td>\n",
       "      <td>1.095594</td>\n",
       "      <td>84.291188</td>\n",
       "      <td>152.949232</td>\n",
       "      <td>102.425147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11367</th>\n",
       "      <td>7/16/2024</td>\n",
       "      <td>1.0894</td>\n",
       "      <td>1.0905</td>\n",
       "      <td>1.0872</td>\n",
       "      <td>1.0899</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>1.08473</td>\n",
       "      <td>-212.509840</td>\n",
       "      <td>67.768595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004143</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.006371</td>\n",
       "      <td>1.072300</td>\n",
       "      <td>1.063738</td>\n",
       "      <td>1.093667</td>\n",
       "      <td>1.095752</td>\n",
       "      <td>47.488584</td>\n",
       "      <td>126.651530</td>\n",
       "      <td>94.120244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11368</th>\n",
       "      <td>7/17/2024</td>\n",
       "      <td>1.0899</td>\n",
       "      <td>1.0948</td>\n",
       "      <td>1.0895</td>\n",
       "      <td>1.0939</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>-0.003839</td>\n",
       "      <td>1.08626</td>\n",
       "      <td>-238.912644</td>\n",
       "      <td>68.292683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004955</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.006533</td>\n",
       "      <td>1.073873</td>\n",
       "      <td>1.063612</td>\n",
       "      <td>1.095410</td>\n",
       "      <td>1.096080</td>\n",
       "      <td>147.465438</td>\n",
       "      <td>143.407029</td>\n",
       "      <td>126.478733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11369</th>\n",
       "      <td>7/18/2024</td>\n",
       "      <td>1.0939</td>\n",
       "      <td>1.0941</td>\n",
       "      <td>1.0894</td>\n",
       "      <td>1.0897</td>\n",
       "      <td>-0.003839</td>\n",
       "      <td>-0.001377</td>\n",
       "      <td>1.08711</td>\n",
       "      <td>80.280638</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>-0.000917</td>\n",
       "      <td>1.076755</td>\n",
       "      <td>1.063587</td>\n",
       "      <td>1.095062</td>\n",
       "      <td>1.096289</td>\n",
       "      <td>35.127911</td>\n",
       "      <td>112.687632</td>\n",
       "      <td>107.956277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11370</th>\n",
       "      <td>7/19/2024</td>\n",
       "      <td>1.0897</td>\n",
       "      <td>1.0902</td>\n",
       "      <td>1.0876</td>\n",
       "      <td>1.0882</td>\n",
       "      <td>-0.001377</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>1.08753</td>\n",
       "      <td>-12.644736</td>\n",
       "      <td>44.117647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>-0.001102</td>\n",
       "      <td>1.078636</td>\n",
       "      <td>1.063566</td>\n",
       "      <td>1.094781</td>\n",
       "      <td>1.096400</td>\n",
       "      <td>-86.743617</td>\n",
       "      <td>81.960318</td>\n",
       "      <td>83.493634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11371 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Dates  PX_OPEN  PX_HIGH  PX_LOW  PX_LAST  Last_Return  \\\n",
       "0      5/27/1980   1.4725   1.4725  1.4725   1.4725     0.008354   \n",
       "1      5/28/1980   1.4676   1.4676  1.4676   1.4676    -0.003328   \n",
       "2      5/29/1980   1.4663   1.4663  1.4663   1.4663    -0.000886   \n",
       "3      5/30/1980   1.4646   1.4646  1.4646   1.4646    -0.001159   \n",
       "4       6/2/1980   1.4631   1.4631  1.4631   1.4631    -0.001024   \n",
       "...          ...      ...      ...     ...      ...          ...   \n",
       "11366  7/15/2024   1.0903   1.0922  1.0882   1.0894    -0.001192   \n",
       "11367  7/16/2024   1.0894   1.0905  1.0872   1.0899     0.000459   \n",
       "11368  7/17/2024   1.0899   1.0948  1.0895   1.0939     0.003670   \n",
       "11369  7/18/2024   1.0939   1.0941  1.0894   1.0897    -0.003839   \n",
       "11370  7/19/2024   1.0897   1.0902  1.0876   1.0882    -0.001377   \n",
       "\n",
       "       Predict_Return   SMA_10   EMA_5_100       RSI_5  ...  MACD_12_5  \\\n",
       "0           -0.003328  1.45190 -691.163309  100.000000  ...   0.007195   \n",
       "1           -0.000886  1.45396   -1.090883   56.379822  ...   0.007560   \n",
       "2           -0.001159  1.45564 -335.993816   75.000000  ...   0.007066   \n",
       "3           -0.001024  1.45715 -752.049206   82.247191  ...   0.006509   \n",
       "4            0.000410  1.45876  292.048549    0.000000  ...   0.006710   \n",
       "...               ...      ...         ...         ...  ...        ...   \n",
       "11366        0.000459  1.08319  -87.689132   70.676692  ...   0.004461   \n",
       "11367        0.003670  1.08473 -212.509840   67.768595  ...   0.004143   \n",
       "11368       -0.003839  1.08626 -238.912644   68.292683  ...   0.004955   \n",
       "11369       -0.001377  1.08711   80.280638   45.000000  ...   0.003988   \n",
       "11370       -0.000092  1.08753  -12.644736   44.117647  ...   0.002381   \n",
       "\n",
       "       MACD_42_18     ROC_5  Bollinger_Bands_lower_12  \\\n",
       "0        0.024641  0.021151                  1.432406   \n",
       "1        0.024186  0.009701                  1.434823   \n",
       "2        0.024720  0.008529                  1.435234   \n",
       "3        0.027378  0.002945                  1.436500   \n",
       "4        0.028643 -0.006384                  1.437667   \n",
       "...           ...       ...                       ...   \n",
       "11366   -0.000664  0.007491                  1.069854   \n",
       "11367   -0.000010  0.006371                  1.072300   \n",
       "11368    0.001334  0.006533                  1.073873   \n",
       "11369    0.001838 -0.000917                  1.076755   \n",
       "11370    0.001704 -0.001102                  1.078636   \n",
       "\n",
       "       Bollinger_Bands_lower_100  Bollinger_Bands_upper_12  \\\n",
       "0                       1.354205                  1.468094   \n",
       "1                       1.354549                  1.470910   \n",
       "2                       1.354825                  1.473583   \n",
       "3                       1.355177                  1.475250   \n",
       "4                       1.355439                  1.476350   \n",
       "...                          ...                       ...   \n",
       "11366                   1.063786                  1.093012   \n",
       "11367                   1.063738                  1.093667   \n",
       "11368                   1.063612                  1.095410   \n",
       "11369                   1.063587                  1.095062   \n",
       "11370                   1.063566                  1.094781   \n",
       "\n",
       "       Bollinger_Bands_upper_100       CCI_5      CCI_22     CCI_100  \n",
       "0                       1.549607  134.258485  274.066780   34.043608  \n",
       "1                       1.548261   59.227299  191.132469   27.108459  \n",
       "2                       1.547053   25.847759  154.692263   26.016046  \n",
       "3                       1.545657  -36.228721  118.693172   24.332424  \n",
       "4                       1.544441  -95.728255   93.544744   22.846588  \n",
       "...                          ...         ...         ...         ...  \n",
       "11366                   1.095594   84.291188  152.949232  102.425147  \n",
       "11367                   1.095752   47.488584  126.651530   94.120244  \n",
       "11368                   1.096080  147.465438  143.407029  126.478733  \n",
       "11369                   1.096289   35.127911  112.687632  107.956277  \n",
       "11370                   1.096400  -86.743617   81.960318   83.493634  \n",
       "\n",
       "[11371 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# sys.path.append(\"./factors\")\n",
    "# import tech_indicators\n",
    "from factors import tech_indicators\n",
    "\n",
    "EURUSD_close = df_tech['PX_LAST'].to_numpy()\n",
    "EURUSD_typical = ((df_tech['PX_HIGH']+df_tech['PX_LOW']+df_tech['PX_LAST'])/3).to_numpy()\n",
    "\n",
    "tech_dict =( {\"SMA\":[[10]],\"EMA\":[[5,100]],\"RSI\":[[5],[22],[100]],\"MACD\":[[26,12],[12,5],[42,18]],\"ROC\":[[5]],\"Bollinger_Bands_lower\":[[12],[100]],\n",
    "             \"Bollinger_Bands_upper\":[[12],[100]],\n",
    "              \"CCI\":[[5],[22],[100]]} )\n",
    "\n",
    "for key,value in tech_dict.items():\n",
    "    \n",
    "    for value in value:\n",
    "        df_column_name = key\n",
    "        for para in value:\n",
    "            df_column_name += (\"_\"+str(para))\n",
    "        \n",
    "        if key[:3] == \"CCI\":\n",
    "            df_tech[df_column_name] = tech_indicators.techindicator_to_array(EURUSD_typical,getattr(tech_indicators,key),*value)\n",
    "            \n",
    "        else:\n",
    "            df_tech[df_column_name] = tech_indicators.techindicator_to_array(EURUSD_close,getattr(tech_indicators,key),*value)\n",
    "        \n",
    "df_tech.dropna(inplace=True,ignore_index=True)\n",
    "\n",
    "\n",
    "df_tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002152\n"
     ]
    }
   ],
   "source": [
    "split_point = int(df_tech.shape[0]*0.9)\n",
    "df_train = df_tech.iloc[:split_point,:]\n",
    "df_test = df_tech.iloc[split_point:,:]\n",
    "\n",
    "x_train = df_train.drop(columns=[\"Dates\",\"Predict_Return\"]).to_numpy()\n",
    "y_train = df_train.loc[:, \"Predict_Return\"].to_numpy()\n",
    "\n",
    "x_test = df_test.drop(columns=[\"Dates\",\"Predict_Return\"]).to_numpy()\n",
    "y_test = df_test.loc[:, \"Predict_Return\"].to_numpy()\n",
    "\n",
    "scale_x = StandardScaler()\n",
    "\n",
    "x_train_norm = scale_x.fit_transform(x_train)\n",
    "x_test_norm = scale_x.transform(x_test)\n",
    "\n",
    "time_delta = prediction_parameters_dic[\"time_rolling_window\"]\n",
    "x_train_norm_rolling,x_test_norm_rolling = rolling_split(x_train_norm,time_delta),rolling_split(x_test_norm,time_delta)\n",
    "y_train_rolling,y_test_rolling = y_train[time_delta-1:,...],y_test[time_delta-1:,...]\n",
    "\n",
    "threshold = threshold_search(y_train,1e-6)\n",
    "\n",
    "print(threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rolling_label , y_test_rolling_label  = labelize(y_train_rolling,threshold),labelize(y_test_rolling,threshold) \n",
    "\n",
    "train_set = torch.utils.data.TensorDataset(torch.from_numpy(x_train_norm_rolling),torch.from_numpy(y_train_rolling_label).to(torch.int64))\n",
    "\n",
    "test_set = torch.utils.data.TensorDataset(torch.from_numpy(x_test_norm_rolling),torch.from_numpy(y_test_rolling_label).to(torch.int64))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuki/miniconda3/envs/initial/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.75 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "import time_net\n",
    "\n",
    "hyperparas = {'input_dim':x_train.shape[-1],'hidden_dim':32,'hidden_nums':5,'output_dim':3,'block_layer_nums':2, 'LSTM_layer_nums':1\n",
    "        , 'dropout_rate':0.75}\n",
    "\n",
    "net_test = time_net.LSTM_Net(hyperparas=hyperparas)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "net_test.to(device=device,dtype=torch.float64)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train_epoch(loss_function, optimizer, model, loader,train_data,test_data):\n",
    "  loss_train = 0\n",
    "  loss_test = 0\n",
    "  \n",
    "  for(i, (x, y)) in enumerate(loader):\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x=x.to(device=device)\n",
    "    y=y.to(device=device)\n",
    "    # Run a forward pass\n",
    "    outputs = model.forward(x)\n",
    "    # Compute the batch loss\n",
    "    loss = loss_function(outputs,y)\n",
    "    # Calculate the gradients\n",
    "    loss.backward()\n",
    "    # Update the parameteres\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "  with torch.no_grad():\n",
    "    train_outputs = model.forward(train_data[:][0].to(device=device))\n",
    "    train_loss = loss_function(train_outputs,train_data[:][1].to(device=device))\n",
    "    loss_train = train_loss.detach().cpu()\n",
    "    print(f\"train loss is {train_loss}\")\n",
    "    \n",
    "    test_outputs = model.forward(test_data[:][0].to(device=device))\n",
    "    test_loss = loss_function(test_outputs,test_data[:][1].to(device=device))\n",
    "    loss_test = test_loss.detach().cpu()\n",
    "    print(f\"test loss is {test_loss}\")\n",
    "        \n",
    "  return [loss_train, loss_test]   \n",
    "\n",
    "\n",
    "\n",
    "def train_model(loss_function, optimizer, model, loader,train_data,test_data,epochs=25):\n",
    "  loss_ls = []\n",
    "  \n",
    "  for i in range(epochs):\n",
    "    print(f\"-----------------------Epoch: {i+1}----------------------------------\")\n",
    "\n",
    "    loss_ls.append(train_epoch(loss_function, optimizer, model, loader,train_data,test_data))\n",
    "    \n",
    "  return loss_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Epoch: 1----------------------------------\n",
      "train loss is 1.5840478158466569\n",
      "test loss is 1.5505048287942844\n",
      "-----------------------Epoch: 2----------------------------------\n",
      "train loss is 1.4142409414754373\n",
      "test loss is 1.3873424624331572\n",
      "-----------------------Epoch: 3----------------------------------\n",
      "train loss is 1.3181896244826607\n",
      "test loss is 1.3508522032439445\n",
      "-----------------------Epoch: 4----------------------------------\n",
      "train loss is 1.2728650750610189\n",
      "test loss is 1.2631089331143932\n",
      "-----------------------Epoch: 5----------------------------------\n",
      "train loss is 1.2104219142734105\n",
      "test loss is 1.1995927442113168\n",
      "-----------------------Epoch: 6----------------------------------\n",
      "train loss is 1.185533644991895\n",
      "test loss is 1.1771895808735944\n",
      "-----------------------Epoch: 7----------------------------------\n",
      "train loss is 1.1612210422942935\n",
      "test loss is 1.1291886846880739\n",
      "-----------------------Epoch: 8----------------------------------\n",
      "train loss is 1.1457565440168063\n",
      "test loss is 1.1424966068097908\n",
      "-----------------------Epoch: 9----------------------------------\n",
      "train loss is 1.1323257700178944\n",
      "test loss is 1.1205361138481398\n",
      "-----------------------Epoch: 10----------------------------------\n",
      "train loss is 1.1249053387442256\n",
      "test loss is 1.1277711584380197\n",
      "-----------------------Epoch: 11----------------------------------\n",
      "train loss is 1.1174275288384572\n",
      "test loss is 1.1230142628509387\n",
      "-----------------------Epoch: 12----------------------------------\n",
      "train loss is 1.1155160437125256\n",
      "test loss is 1.1054354909547022\n",
      "-----------------------Epoch: 13----------------------------------\n",
      "train loss is 1.1130052888839317\n",
      "test loss is 1.1058932825224888\n",
      "-----------------------Epoch: 14----------------------------------\n",
      "train loss is 1.109863954822244\n",
      "test loss is 1.1062663902154544\n",
      "-----------------------Epoch: 15----------------------------------\n",
      "train loss is 1.1030293523811439\n",
      "test loss is 1.1060647233740133\n",
      "-----------------------Epoch: 16----------------------------------\n",
      "train loss is 1.103491214676407\n",
      "test loss is 1.1154616700299087\n",
      "-----------------------Epoch: 17----------------------------------\n",
      "train loss is 1.1035486664807015\n",
      "test loss is 1.1044007919018979\n",
      "-----------------------Epoch: 18----------------------------------\n",
      "train loss is 1.1023925945820712\n",
      "test loss is 1.1042381708652653\n",
      "-----------------------Epoch: 19----------------------------------\n",
      "train loss is 1.0998865592111935\n",
      "test loss is 1.1037306219724519\n",
      "-----------------------Epoch: 20----------------------------------\n",
      "train loss is 1.1009852013791233\n",
      "test loss is 1.1009874345364836\n",
      "-----------------------Epoch: 21----------------------------------\n",
      "train loss is 1.1004813217496696\n",
      "test loss is 1.10101394843703\n",
      "-----------------------Epoch: 22----------------------------------\n",
      "train loss is 1.0984529355195431\n",
      "test loss is 1.106155193603073\n",
      "-----------------------Epoch: 23----------------------------------\n",
      "train loss is 1.0975617402984557\n",
      "test loss is 1.1007190553573662\n",
      "-----------------------Epoch: 24----------------------------------\n",
      "train loss is 1.0984199140131081\n",
      "test loss is 1.1025511688626655\n",
      "-----------------------Epoch: 25----------------------------------\n",
      "train loss is 1.0982501801809557\n",
      "test loss is 1.098639796526011\n",
      "-----------------------Epoch: 26----------------------------------\n",
      "train loss is 1.098129260810125\n",
      "test loss is 1.1053360289507708\n",
      "-----------------------Epoch: 27----------------------------------\n",
      "train loss is 1.0979680706443578\n",
      "test loss is 1.100652601460161\n",
      "-----------------------Epoch: 28----------------------------------\n",
      "train loss is 1.0973090036650006\n",
      "test loss is 1.1036684554452059\n",
      "-----------------------Epoch: 29----------------------------------\n",
      "train loss is 1.0977743838159397\n",
      "test loss is 1.1011216273278965\n",
      "-----------------------Epoch: 30----------------------------------\n",
      "train loss is 1.0961240961289116\n",
      "test loss is 1.1012427120907347\n",
      "-----------------------Epoch: 31----------------------------------\n",
      "train loss is 1.0968604473179537\n",
      "test loss is 1.0952731861585958\n",
      "-----------------------Epoch: 32----------------------------------\n",
      "train loss is 1.0936734755434296\n",
      "test loss is 1.0979431606434473\n",
      "-----------------------Epoch: 33----------------------------------\n",
      "train loss is 1.0941627235123959\n",
      "test loss is 1.1009035328607235\n",
      "-----------------------Epoch: 34----------------------------------\n",
      "train loss is 1.0939039522473577\n",
      "test loss is 1.0955952411450876\n",
      "-----------------------Epoch: 35----------------------------------\n",
      "train loss is 1.0933798586924173\n",
      "test loss is 1.09547150960277\n",
      "-----------------------Epoch: 36----------------------------------\n",
      "train loss is 1.0947840830440974\n",
      "test loss is 1.0982351815107794\n",
      "-----------------------Epoch: 37----------------------------------\n",
      "train loss is 1.0921566487670253\n",
      "test loss is 1.0974017446658015\n",
      "-----------------------Epoch: 38----------------------------------\n",
      "train loss is 1.0924477814220144\n",
      "test loss is 1.0999740051082854\n",
      "-----------------------Epoch: 39----------------------------------\n",
      "train loss is 1.0907028412833923\n",
      "test loss is 1.0942807160596593\n",
      "-----------------------Epoch: 40----------------------------------\n",
      "train loss is 1.0901768573818424\n",
      "test loss is 1.0907379820572765\n",
      "-----------------------Epoch: 41----------------------------------\n",
      "train loss is 1.0890979750386478\n",
      "test loss is 1.0939472686324585\n",
      "-----------------------Epoch: 42----------------------------------\n",
      "train loss is 1.0900925011543314\n",
      "test loss is 1.0906885460816176\n",
      "-----------------------Epoch: 43----------------------------------\n",
      "train loss is 1.0887093300984243\n",
      "test loss is 1.090289981316034\n",
      "-----------------------Epoch: 44----------------------------------\n",
      "train loss is 1.0879433222672643\n",
      "test loss is 1.087431502377472\n",
      "-----------------------Epoch: 45----------------------------------\n",
      "train loss is 1.086604675067755\n",
      "test loss is 1.0893438686436343\n",
      "-----------------------Epoch: 46----------------------------------\n",
      "train loss is 1.087914490097745\n",
      "test loss is 1.092380090196997\n",
      "-----------------------Epoch: 47----------------------------------\n",
      "train loss is 1.0866624953040367\n",
      "test loss is 1.0928670180066886\n",
      "-----------------------Epoch: 48----------------------------------\n",
      "train loss is 1.0871805710922782\n",
      "test loss is 1.0908820372172598\n",
      "-----------------------Epoch: 49----------------------------------\n",
      "train loss is 1.0859959937765358\n",
      "test loss is 1.0868763253181446\n",
      "-----------------------Epoch: 50----------------------------------\n",
      "train loss is 1.0859026268241834\n",
      "test loss is 1.08595063577012\n",
      "-----------------------Epoch: 51----------------------------------\n",
      "train loss is 1.0854207795879847\n",
      "test loss is 1.0920952624091218\n",
      "-----------------------Epoch: 52----------------------------------\n",
      "train loss is 1.0853133266840584\n",
      "test loss is 1.0867205923628285\n",
      "-----------------------Epoch: 53----------------------------------\n",
      "train loss is 1.0856710990409932\n",
      "test loss is 1.0888315381655245\n",
      "-----------------------Epoch: 54----------------------------------\n",
      "train loss is 1.084429338672596\n",
      "test loss is 1.0907580947151072\n",
      "-----------------------Epoch: 55----------------------------------\n",
      "train loss is 1.0844273852585884\n",
      "test loss is 1.0870366518669665\n",
      "-----------------------Epoch: 56----------------------------------\n",
      "train loss is 1.084567084282982\n",
      "test loss is 1.0881384694570184\n",
      "-----------------------Epoch: 57----------------------------------\n",
      "train loss is 1.0838841377271926\n",
      "test loss is 1.0882095460970245\n",
      "-----------------------Epoch: 58----------------------------------\n",
      "train loss is 1.0825022034998997\n",
      "test loss is 1.0878774543974912\n",
      "-----------------------Epoch: 59----------------------------------\n",
      "train loss is 1.0839467353795245\n",
      "test loss is 1.0896345939783683\n",
      "-----------------------Epoch: 60----------------------------------\n",
      "train loss is 1.0827805647245963\n",
      "test loss is 1.0882087068192488\n",
      "-----------------------Epoch: 61----------------------------------\n",
      "train loss is 1.0838384257651963\n",
      "test loss is 1.0817760966273446\n",
      "-----------------------Epoch: 62----------------------------------\n",
      "train loss is 1.0827071214571706\n",
      "test loss is 1.0850771983258578\n",
      "-----------------------Epoch: 63----------------------------------\n",
      "train loss is 1.0827650545984346\n",
      "test loss is 1.0875904285942057\n",
      "-----------------------Epoch: 64----------------------------------\n",
      "train loss is 1.081831591171518\n",
      "test loss is 1.0814969375934327\n",
      "-----------------------Epoch: 65----------------------------------\n",
      "train loss is 1.0820852059760437\n",
      "test loss is 1.0851214521812533\n",
      "-----------------------Epoch: 66----------------------------------\n",
      "train loss is 1.0796118784737163\n",
      "test loss is 1.0853791122593577\n",
      "-----------------------Epoch: 67----------------------------------\n",
      "train loss is 1.0820857261372754\n",
      "test loss is 1.0874451132465954\n",
      "-----------------------Epoch: 68----------------------------------\n",
      "train loss is 1.0811152727332007\n",
      "test loss is 1.0829772994258895\n",
      "-----------------------Epoch: 69----------------------------------\n",
      "train loss is 1.0811214856157012\n",
      "test loss is 1.0840330789831047\n",
      "-----------------------Epoch: 70----------------------------------\n",
      "train loss is 1.0796287997753549\n",
      "test loss is 1.0901260386843912\n",
      "-----------------------Epoch: 71----------------------------------\n",
      "train loss is 1.0789508146307154\n",
      "test loss is 1.0819930348980837\n",
      "-----------------------Epoch: 72----------------------------------\n",
      "train loss is 1.08084150544099\n",
      "test loss is 1.0857007727163592\n",
      "-----------------------Epoch: 73----------------------------------\n",
      "train loss is 1.0803688863918226\n",
      "test loss is 1.0888053140037166\n",
      "-----------------------Epoch: 74----------------------------------\n",
      "train loss is 1.0795437557210668\n",
      "test loss is 1.0819489756580107\n",
      "-----------------------Epoch: 75----------------------------------\n",
      "train loss is 1.0804281216530036\n",
      "test loss is 1.083572170766868\n",
      "-----------------------Epoch: 76----------------------------------\n",
      "train loss is 1.0784497404594766\n",
      "test loss is 1.0846614592503714\n",
      "-----------------------Epoch: 77----------------------------------\n",
      "train loss is 1.0790474939391268\n",
      "test loss is 1.0827931629000285\n",
      "-----------------------Epoch: 78----------------------------------\n",
      "train loss is 1.0783191152922131\n",
      "test loss is 1.0826765075128397\n",
      "-----------------------Epoch: 79----------------------------------\n",
      "train loss is 1.07938882803705\n",
      "test loss is 1.0862177920678524\n",
      "-----------------------Epoch: 80----------------------------------\n",
      "train loss is 1.0787506939483422\n",
      "test loss is 1.0824385733354245\n",
      "-----------------------Epoch: 81----------------------------------\n",
      "train loss is 1.0785953077301524\n",
      "test loss is 1.0789140569826834\n",
      "-----------------------Epoch: 82----------------------------------\n",
      "train loss is 1.0771846660860152\n",
      "test loss is 1.0807660149832874\n",
      "-----------------------Epoch: 83----------------------------------\n",
      "train loss is 1.0789164112271084\n",
      "test loss is 1.0828017779476413\n",
      "-----------------------Epoch: 84----------------------------------\n",
      "train loss is 1.0783292218410678\n",
      "test loss is 1.0834388955579202\n",
      "-----------------------Epoch: 85----------------------------------\n",
      "train loss is 1.0778541249145026\n",
      "test loss is 1.0892974139315743\n",
      "-----------------------Epoch: 86----------------------------------\n",
      "train loss is 1.0767886323613751\n",
      "test loss is 1.0830277065133187\n",
      "-----------------------Epoch: 87----------------------------------\n",
      "train loss is 1.0771783056949804\n",
      "test loss is 1.0846156876036193\n",
      "-----------------------Epoch: 88----------------------------------\n",
      "train loss is 1.077746443278836\n",
      "test loss is 1.0822171184896126\n",
      "-----------------------Epoch: 89----------------------------------\n",
      "train loss is 1.0757433765654603\n",
      "test loss is 1.0833612077191292\n",
      "-----------------------Epoch: 90----------------------------------\n",
      "train loss is 1.0774877982986821\n",
      "test loss is 1.086415314039422\n",
      "-----------------------Epoch: 91----------------------------------\n",
      "train loss is 1.0769420481350185\n",
      "test loss is 1.0842058985157046\n",
      "-----------------------Epoch: 92----------------------------------\n",
      "train loss is 1.0779221791633213\n",
      "test loss is 1.0766013002092547\n",
      "-----------------------Epoch: 93----------------------------------\n",
      "train loss is 1.076553711732462\n",
      "test loss is 1.0779840747929097\n",
      "-----------------------Epoch: 94----------------------------------\n",
      "train loss is 1.0757731616200097\n",
      "test loss is 1.0816506483386408\n",
      "-----------------------Epoch: 95----------------------------------\n",
      "train loss is 1.0766557223189708\n",
      "test loss is 1.0845721039621798\n",
      "-----------------------Epoch: 96----------------------------------\n",
      "train loss is 1.0755380106255867\n",
      "test loss is 1.0789425582784327\n",
      "-----------------------Epoch: 97----------------------------------\n",
      "train loss is 1.0753463446129774\n",
      "test loss is 1.0822114221365058\n",
      "-----------------------Epoch: 98----------------------------------\n",
      "train loss is 1.0756068203500828\n",
      "test loss is 1.0854619719525396\n",
      "-----------------------Epoch: 99----------------------------------\n",
      "train loss is 1.0745967405583592\n",
      "test loss is 1.08579705581909\n",
      "-----------------------Epoch: 100----------------------------------\n",
      "train loss is 1.0747651657658308\n",
      "test loss is 1.0782009752370036\n"
     ]
    }
   ],
   "source": [
    "optim_Adam = torch.optim.Adam(net_test.parameters(),lr = 0.0001)\n",
    "epochs = 100\n",
    "loss_ls = train_model(loss_function=loss,optimizer=optim_Adam,model=net_test,loader=train_loader,train_data=train_set,\n",
    "                      test_data=test_set,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuki/miniconda3/envs/initial/lib/python3.12/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/home/yuki/miniconda3/envs/initial/lib/python3.12/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_df = pd.DataFrame(data=np.array(loss_ls),columns=[\"Train_loss\", \"Test_loss\"])\n",
    "seaborn.lineplot(data=loss_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41718889883616833"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "net_test.to(device=torch.device(\"cpu\"))\n",
    "net_test.eval()\n",
    "\n",
    "predict_probability = torch.max(F.softmax(net_test(test_set[:][0]),dim=-1),dim=-1)[0]\n",
    "\n",
    "predict_results = torch.argmax(F.softmax(net_test(test_set[:][0]),dim=-1),dim=-1)\n",
    "real_results = test_set[:][1]\n",
    "\n",
    "accuracy_score(real_results,predict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34934497816593885"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_predict_bool = (predict_results != 1) & (predict_probability > 0.35)\n",
    "\n",
    "act_predict_results = predict_results[act_predict_bool]\n",
    "act_real_results = real_results[act_predict_bool]\n",
    "\n",
    "accuracy_score(act_predict_results,act_real_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([229])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_predict_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_path = \"../trained_model/Tec_LSTM.pth\"\n",
    "\n",
    "torch.save(net_test.state_dict(),saved_path )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
